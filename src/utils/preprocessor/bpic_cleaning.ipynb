{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv\n",
    "import psutil\n",
    "import time\n",
    "# from memory_profiler import profile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logs = [\n",
    "        'bpic2012',\n",
    "        'bpic2017', \n",
    "        #'trafficFines',\n",
    "        ]\n",
    "\n",
    "\n",
    "# standardize column names: ['case_id', 'activity', 'resource', 'timestamp'] for all logs\n",
    "case_id_col = 'case_nr'\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "\n",
    "# dataset_name: [case_id_col, activity_col, resource_col, timestamp_col]\n",
    "# NOTE - Basic column names are defined by users. These names will be renamed and standardized later \n",
    "# standardize column names: ['case_id', 'activity', 'resource', 'timestamp']\n",
    "dataset_dict = {\n",
    "    'bpic2012': ['case_id', 'activity', 'resource', 'start_time'],\n",
    "    'bpic2017': ['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp'],\n",
    "    #'trafficFines': ['case:concept:name', 'concept:name', 'org:resource', \"time:timestamp\"], \n",
    "}\n",
    "incomplete_dict = {\n",
    "    'bpic2012': [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\", \"A_CANCELLED\", \"A_DECLINED\"],\n",
    "    'bpic2017': [\"A_Pending\", \"A_Denied\", \"A_Cancelled\",],\n",
    "    #'trafficFines': [\"Send for Credit Collection\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Define positive and negative labels\n",
    "label_old = \"outcome\"\n",
    "neg_label = 0\n",
    "pos_label = 1\n",
    "positive_activities_dict = {\n",
    "    'bpic2017': [\"A_Pending\"],\n",
    "    'bpic2012': [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\",],\n",
    "    #'trafficFines': [\"Send for Credit Collection\"]\n",
    "}\n",
    "\n",
    "chunk_s = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessorModinDask:\n",
    "    def __init__(self, log_name, input_data_path):\n",
    "        self.log_name = log_name\n",
    "        self.input_data_path = input_data_path\n",
    "\n",
    "    def _print_metrics(self, elapsed_time):\n",
    "        # Helper function to print resource usage metrics\n",
    "        cpu_percent = psutil.cpu_percent()\n",
    "        memory_info = psutil.virtual_memory()\n",
    "\n",
    "    # @profile\n",
    "    def read_log(self):\n",
    "        start_time = time.time()\n",
    "        print(\"Reading log...\")\n",
    "\n",
    "        # Read the log based on its extension\n",
    "        if self.input_data_path.lower().endswith('.csv'):\n",
    "            with open(self.input_data_path, 'r') as file:\n",
    "                # Use Sniffer to infer the delimiter\n",
    "                dialect = csv.Sniffer().sniff(file.read(10000))\n",
    "            try:\n",
    "                # Read CSV with Dask, specifying dtype for certain columns\n",
    "                log_file = pd.read_csv(self.input_data_path, sep=dialect.delimiter, dtype={'Resource': 'object', 'article': 'object'})\n",
    "            except:\n",
    "                log_file = pd.read_csv(self.input_data_path, sep=dialect.delimiter)\n",
    "\n",
    "            log_file = log_file.rename(columns=lambda x: x.strip().lower().replace(' ', '_'))\n",
    "            log_file = log_file.rename(\n",
    "                columns=dict(zip(dataset_dict[self.log_name], [case_id_col, activity_col, resource_col, timestamp_col])))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file extension. Supported extensions: .csv, .xes\")\n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "        return log_file\n",
    "\n",
    "    # @profile\n",
    "    def clean_data(self, log_file):\n",
    "        start_time = time.time()\n",
    "        print(\"Cleaning data...\")\n",
    "\n",
    "        # Convert timestamp column to datetime\n",
    "        log_file[timestamp_col] = pd.to_datetime(log_file[timestamp_col], format=\"mixed\", infer_datetime_format=True)\n",
    "        log_file = log_file.sort_values(by=[timestamp_col])\n",
    "\n",
    "       # Remove white spaces from column values\n",
    "        log_file = log_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "        # Convert the 'resource' column to string\n",
    "        log_file[resource_col] = log_file[resource_col].astype(str)\n",
    "\n",
    "        # Replace unique resource values with 'res{i}' format\n",
    "        unique_resources = log_file[resource_col].unique()\n",
    "        resource_mapping = {original_value: f'res{i + 1}' for i, original_value in enumerate(unique_resources)}\n",
    "        log_file[resource_col] = log_file[resource_col].replace(resource_mapping)\n",
    "\n",
    "        # Make sure that a 0 in the following columns is replaced by NaN\n",
    "        if self.log_name == 'bpic2017':\n",
    "            columns_to_check = ['firstwithdrawalamount', 'monthlycost', 'creditscore', 'offeredamount', \"numberofterms\"]\n",
    "            log_file[columns_to_check] = log_file[columns_to_check].replace(0, pd.NA)\n",
    "\n",
    "        # There are some columns that have NaN up until a certain point in a case, then they are filled in one row. Make sure that they have the same value starting from the first occurrence up until the next change\n",
    "        filled_file = log_file.groupby(case_id_col, group_keys=False).ffill()\n",
    "        # make sure to have column case_id_col still in the dataframe\n",
    "        filled_file[case_id_col] = log_file[case_id_col]\n",
    "        log_file = filled_file\n",
    "\n",
    "        # activities_to_check = incomplete_dict[log_name]  # [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\", \"A_CANCELLED\", \"A_DECLINED\"]\n",
    "        activities_to_check = incomplete_dict[self.log_name]  # [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\", \"A_CANCELLED\", \"A_DECLINED\"]              \n",
    "        contains_activity = log_file[activity_col].isin(activities_to_check).groupby(log_file[case_id_col]).max().reset_index()\n",
    "        complete_cases = contains_activity[contains_activity[activity_col] == True][case_id_col]\n",
    "        log_file = log_file[log_file[case_id_col].isin(complete_cases.tolist())]       \n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "        return log_file\n",
    "    \n",
    "    # @profile\n",
    "    def extract_temporal_features(self, log_file):\n",
    "        start_time = time.time()\n",
    "        print(\"Extracting timestamp features...\")\n",
    "\n",
    "        # Calculate event_nr\n",
    "        log_file['event_nr'] = log_file.groupby(case_id_col).cumcount() + 1\n",
    "\n",
    "        # Calculate case_length\n",
    "        log_file['case_length'] = log_file.groupby(case_id_col)['event_nr'].transform('max')\n",
    "\n",
    "        log_file['elapsed_time'] = log_file[timestamp_col] - log_file.groupby(case_id_col)[timestamp_col].transform('min')\n",
    "        log_file['elapsed_time'] = log_file['elapsed_time'].dt.total_seconds()\n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "\n",
    "        return log_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      " Log: bpic2012\n",
      "==================\n",
      "\n",
      "Reading log...\n",
      "Cleaning data...\n",
      "Extracting timestamp features...\n",
      "Saving csv file...\n",
      "Done!\n",
      "\n",
      "==================\n",
      " Log: bpic2017\n",
      "==================\n",
      "\n",
      "Reading log...\n",
      "Cleaning data...\n",
      "Extracting timestamp features...\n",
      "Saving csv file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "results_data = []\n",
    "# Initialize your DataPreprocessorModinDask object and call read_log() and clean_data()\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "    input_data_path = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\%s\\\\%s.csv\" % (log_name, log_name)\n",
    "    output_data_path = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\prepared_data\\\\%s\" % log_name\n",
    "\n",
    "    data_preprocessor = DataPreprocessorModinDask(log_name, input_data_path=input_data_path)\n",
    "    log_file = data_preprocessor.read_log()\n",
    "    cleaned_data = data_preprocessor.clean_data(log_file)\n",
    "    features_data = data_preprocessor.extract_temporal_features(cleaned_data)\n",
    "    features_data.name = \"%s_cleaned.csv\" % log_name\n",
    "    results_data.append(features_data)\n",
    "    \n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\%s\" % log_name\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "    features_data.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, features_data.name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Interventions For bpic2017 from the winner student report:\n",
    "\n",
    "1. **Sending Another Loan Offer:**\n",
    "   - *Intervention:* Send offers to clients as soon as possible. For all case endpoints, this has been shown to have the greatest effect on cancellation rates. Sending offers to clients within 4 days may decrease cancellation rates by 5% up to 10%.\n",
    "   - *Treatment 1:* Cases that receive only one offer are in the control group, while cases that receive more than one offer are in the treatment group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      " Log: bpic2012\n",
      "==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:   1%|          | 75/12688 [00:00<00:17, 741.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  25%|██▍       | 3169/12688 [00:03<00:11, 850.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  48%|████▊     | 6119/12688 [00:07<00:08, 767.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  72%|███████▏  | 9177/12688 [00:11<00:04, 857.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  95%|█████████▌| 12104/12688 [00:15<00:00, 785.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups: 100%|██████████| 12688/12688 [00:16<00:00, 775.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving csv file...\n",
      "Done!\n",
      "\n",
      "\n",
      "==================\n",
      " Log: bpic2017\n",
      "==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:   0%|          | 78/31411 [00:00<02:05, 250.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  10%|▉         | 3139/31411 [00:04<00:39, 723.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  19%|█▉        | 6119/31411 [00:09<00:41, 615.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  29%|██▉       | 9105/31411 [00:13<00:30, 743.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  39%|███▊      | 12152/31411 [00:18<00:27, 710.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  48%|████▊     | 15135/31411 [00:22<00:21, 746.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  58%|█████▊    | 18105/31411 [00:26<00:20, 646.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  67%|██████▋   | 21076/31411 [00:30<00:13, 777.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  21000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  77%|███████▋  | 24143/31411 [00:35<00:10, 720.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  86%|████████▋ | 27107/31411 [00:39<00:06, 690.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  27000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups:  96%|█████████▌| 30150/31411 [00:44<00:01, 682.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling groups: 100%|██████████| 31411/31411 [00:46<00:00, 680.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving csv file...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_17 = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\bpic2017\\\\bpic2017_cleaned.csv\"\n",
    "path_12 = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\bpic2012\\\\bpic2012_cleaned.csv\"\n",
    "\n",
    "results_data = [pd.read_csv(path_12, sep=\";\"), pd.read_csv(path_17, sep=\";\")]\n",
    "\n",
    "results_data2 = []\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "    positive_activities = positive_activities_dict[log_name]\n",
    "\n",
    "    # Initialize a global state to keep track of positive activities across cases\n",
    "    global_positive_cases = set()\n",
    "\n",
    "    # Define a function to label each group\n",
    "    def label_group(chunk):\n",
    "        # Check if any positive activity exists in the case\n",
    "        chunk['is_positive'] = chunk.groupby(case_id_col)[activity_col].transform(lambda x: any(activity in positive_activities for activity in x))\n",
    "        \n",
    "        # Update the global positive cases state\n",
    "        positive_cases_from_chunk = set(chunk.loc[chunk['is_positive'], case_id_col])\n",
    "        global_positive_cases.update(positive_cases_from_chunk)\n",
    "        \n",
    "        # Assign label based on the result\n",
    "        chunk['outcome'] = chunk.apply(lambda row: pos_label if row['is_positive'] else neg_label, axis=1)\n",
    "        \n",
    "        return chunk.drop('is_positive', axis=1)\n",
    "\n",
    "    features_data = results_data[logs.index(log_name)]\n",
    "\n",
    "    grouped = features_data.groupby(case_id_col, as_index=False)\n",
    "    c = 0\n",
    "    results = []\n",
    "    for name, group in tqdm(grouped, desc=\"Labeling groups\"):\n",
    "        if c % 3000 == 0:\n",
    "            print(\"This is chunk number: \", c)\n",
    "        # Reset the index for each group\n",
    "        group.reset_index(drop=True, inplace=True)\n",
    "        labeled_group = label_group(group)\n",
    "        results.append(labeled_group)\n",
    "        c += 1\n",
    "\n",
    "    # Concatenate the results of all chunks into a final DataFrame\n",
    "    labeled_data = pd.concat(results, ignore_index=True)\n",
    "    results_data2.append(labeled_data)\n",
    "\n",
    "    labeled_data.name = \"%s_labeled.csv\" % log_name\n",
    "\n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\%s\" % log_name\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "    labeled_data.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, labeled_data.name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "    print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      " Log: bpic2012\n",
      "==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying Treatments:   0%|          | 0/12688 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying Treatments:  41%|████▏     | 5240/12688 [00:04<00:05, 1370.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying Treatments:  79%|███████▊  | 9968/12688 [00:07<00:01, 1392.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying Treatments: 100%|██████████| 12688/12688 [00:09<00:00, 1298.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! - Final DataFrame:\n",
      "\n",
      "Saving csv file...\n",
      "Done!\n",
      "\n",
      "\n",
      "==================\n",
      " Log: bpic2017\n",
      "==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying Treatments:   0%|          | 1/31411 [00:00<1:18:28,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is chunk number:  0\n",
      "Case 1001114274\n",
      "0       A_Create Application\n",
      "1                A_Submitted\n",
      "2             W_Handle leads\n",
      "3             W_Handle leads\n",
      "4     W_Complete application\n",
      "               ...          \n",
      "64    W_Validate application\n",
      "65               O_Cancelled\n",
      "66               O_Cancelled\n",
      "67               O_Cancelled\n",
      "68               O_Cancelled\n",
      "Name: activity, Length: 69, dtype: object\n",
      "[10, 18, 37, 48, 51]\n",
      "[12, 20, 39, 50, 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying Treatments:   0%|          | 18/31411 [01:50<53:19:45,  6.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Reset the index for each group\u001b[39;00m\n\u001b[0;32m     98\u001b[0m group\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 99\u001b[0m treated_chunk \u001b[38;5;241m=\u001b[39m treatment_function(group\u001b[38;5;241m.\u001b[39mcopy(), log_name)\n\u001b[0;32m    100\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(treated_chunk)\n\u001b[0;32m    101\u001b[0m c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m, in \u001b[0;36mapply_treatment1\u001b[1;34m(group, log_name)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(indices)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(indices_offer_sent)\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlol\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices_offer_sent) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# put it one event before the second offer, as the decision is made before the offer is sent\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# check whether the timestamp of the first offer sent is before the timestamp of the second offer\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group\u001b[38;5;241m.\u001b[39mat[indices_offer_sent[\u001b[38;5;241m0\u001b[39m], timestamp_col] \u001b[38;5;241m<\u001b[39m group\u001b[38;5;241m.\u001b[39mat[indices[\u001b[38;5;241m1\u001b[39m], timestamp_col]:\n",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m, in \u001b[0;36mapply_treatment1\u001b[1;34m(group, log_name)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(indices)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(indices_offer_sent)\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlol\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices_offer_sent) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# put it one event before the second offer, as the decision is made before the offer is sent\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# check whether the timestamp of the first offer sent is before the timestamp of the second offer\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group\u001b[38;5;241m.\u001b[39mat[indices_offer_sent[\u001b[38;5;241m0\u001b[39m], timestamp_col] \u001b[38;5;241m<\u001b[39m group\u001b[38;5;241m.\u001b[39mat[indices[\u001b[38;5;241m1\u001b[39m], timestamp_col]:\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\u0166838\\AppData\\Local\\anaconda3\\envs\\PresPMGenerator\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\u0166838\\AppData\\Local\\anaconda3\\envs\\PresPMGenerator\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_17 = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\bpic2017\\\\bpic2017_labeled.csv\"\n",
    "path_12 = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\bpic2012\\\\bpic2012_labeled.csv\"\n",
    "\n",
    "results_data2 = [pd.read_csv(path_12, sep=\";\"), pd.read_csv(path_17, sep=\";\")]\n",
    "\n",
    "results_data3 = []\n",
    "\n",
    "relevant_activities_dict = {\n",
    "    \"bpic2017\": ['O_Sent (mail and online)', 'O_Sent (online only)'],\n",
    "    \"bpic2012\": ['O_SENT'],\n",
    "    \"trafficFines\": ['Add penalty']\n",
    "    }\n",
    "\n",
    "# Function for Treatment 1: Increase the number of offers\n",
    "def apply_treatment1(group, log_name):\n",
    "    if log_name == \"bpic2017\":     \n",
    "        group['treatment'] = 0\n",
    "\n",
    "        # IMPORTANT NOTE: IN PREVIOUS PAPERS --> INTERVENTION IS SENDING MULTIPLE OFFERS, HOWEVER, THE DECISION IS ALREADY MADE WHEN THAT OFFER IS CREATED FOR BPIC2017 !\n",
    "        # the offer is not always sent when Create Offer occurs, but this is the actual treatment we are interested, we just have to make sure that the previous offer was already sent to the client\n",
    "\n",
    "        # indices = group.index[(group[activity_col] == 'O_Sent (mail and online)') | (group[activity_col] == 'O_Sent (online only)')].tolist()\n",
    "        indices = group.index[(group[activity_col] == 'O_Create Offer')].tolist()\n",
    "        indices_offer_sent = group.index[(group[activity_col] == 'O_Sent (mail and online)') | (group[activity_col] == 'O_Sent (online only)')].tolist()\n",
    "        # check the case_nr\n",
    "        if group[case_id_col].iloc[0] == \"Application_1001114274\":\n",
    "            print(\"Case 1001114274\")\n",
    "            print(group[activity_col])\n",
    "            print(indices)\n",
    "            print(indices_offer_sent)\n",
    "            print('lol')\n",
    "        if len(indices) > 1 and len(indices_offer_sent) > 0:\n",
    "            # put it one event before the second offer, as the decision is made before the offer is sent\n",
    "            # check whether the timestamp of the first offer sent is before the timestamp of the second offer\n",
    "            if group.at[indices_offer_sent[0], timestamp_col] < group.at[indices[1], timestamp_col]:\n",
    "                group.at[indices[1]-1, 'treatment'] = 1\n",
    "\n",
    "        # Reset the index for each group\n",
    "        group.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return group\n",
    "\n",
    "    elif log_name == \"bpic2012\":\n",
    "        group[\"treatment\"] = 0\n",
    "        # indices = group.index[(group[activity_col] == 'O_SENT')].tolist()\n",
    "\n",
    "        # IMPORTANT NOTE: IN PREVIOUS PAPERS --> INTERVENTION IS SENDING MULTIPLE OFFERS, HOWEVER, THE DECISION IS ALREADY MADE WHEN THAT OFFER IS SELECTED FOR BPIC2012 ! \n",
    "        # We just also make sure that the previous offer was also sent to the client (otherwise it's also the 'first offer')\n",
    "\n",
    "        # SO DECISION IS: WE SENT AN OFFER, THE CLIENT REFUSED, SO DO WE CREATE A NEW OFFER OR NOT?\n",
    "\n",
    "        indices = group.index[(group[activity_col] == 'O_SELECTED')].tolist()\n",
    "        indices_offer_sent = group.index[(group[activity_col] == 'O_SENT')].tolist()\n",
    "        if len(indices) > 1 and len(indices_offer_sent) > 0:\n",
    "            # put it one event before the second offer, as the decision is made before the offer is sent\n",
    "            # check whether the timestamp of the first offer sent is before the timestamp of the second offer\n",
    "            if group.at[indices_offer_sent[0], timestamp_col] < group.at[indices[1], timestamp_col]:\n",
    "                group.at[indices[1]-1, 'treatment'] = 1\n",
    "        \n",
    "        # Reset the index for each group\n",
    "        group.reset_index(drop=True, inplace=True)\n",
    "        return group\n",
    "\n",
    "\n",
    "treatments_functions = [apply_treatment1,] # apply_treatment2, apply_treatment3, apply_treatment4]\n",
    "\n",
    "# Set the maximum number of cases per chunk\n",
    "max_cases_per_chunk = 100\n",
    "\n",
    "# List to store futures\n",
    "futures = []\n",
    "\n",
    "max_workers = 1  # You can adjust this number based on your system's capabilities\n",
    "\n",
    "def determine_overall_treatment(row):\n",
    "    treatments = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\n",
    "    for treatment in treatments:\n",
    "        if row[treatment] == 'Treatment':\n",
    "            return treatment\n",
    "    return 'Controle'\n",
    "\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))    \n",
    "    relevant_activities = relevant_activities_dict[log_name]\n",
    "    labeled_data = results_data2[logs.index(log_name)]\n",
    "    futures = []\n",
    "    results = []\n",
    "    for treatment_function in treatments_functions:\n",
    "        grouped = labeled_data.groupby(case_id_col, as_index=False)\n",
    "        c = 0\n",
    "        for name, group in tqdm(grouped, desc=\"Applying Treatments\"):\n",
    "            if c % 5000 == 0:\n",
    "                print(\"This is chunk number: \", c)\n",
    "            # Reset the index for each group\n",
    "            group.reset_index(drop=True, inplace=True)\n",
    "            treated_chunk = treatment_function(group.copy(), log_name)\n",
    "            results.append(treated_chunk)\n",
    "            c += 1\n",
    "    final_result = pd.concat(results, ignore_index=True)\n",
    "    final_result.reset_index(drop=True, inplace=True)  # Resetting index to avoid duplicate indices\n",
    "    # add time to last event column\n",
    "    activities_to_track = incomplete_dict[log_name]\n",
    "\n",
    "    # Display the final result\n",
    "    print(\"\\nDone! - Final DataFrame:\\n\")\n",
    "    final_result.name = \"%s_treatments.csv\" % log_name\n",
    "\n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\%s\" % log_name\n",
    "    import os\n",
    "\n",
    "    results_data3.append(final_result)\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "    final_result.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, final_result.name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "    print(\"Done!\\n\")\n",
    "    final_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   resource  case_count\n",
      "0      res1       12688\n",
      "22     res3        3513\n",
      "42    res48        1689\n",
      "34    res40        1665\n",
      "21    res29        1642\n",
      "['res1' 'res3' 'other' 'res48' 'res40' 'res29']\n",
      "    resource  case_count\n",
      "0       res1       23377\n",
      "104     res6        6052\n",
      "62     res21        5801\n",
      "56     res16        4784\n",
      "78     res36        4403\n",
      "['res1' 'other' 'res21' 'res6' 'res36' 'res16']\n",
      "                 case_goal  case_count\n",
      "0                     Boat         201\n",
      "1            Business goal          30\n",
      "2                      Car        9307\n",
      "3         Caravan / Camper         369\n",
      "4       Debt restructuring           2\n",
      "5   Existing loan takeover        5574\n",
      "6     Extra spending limit         623\n",
      "7         Home improvement        7646\n",
      "8               Motorcycle         275\n",
      "9   Other, see explanation        2976\n",
      "10     Remaining debt home         835\n",
      "11            Tax payments         152\n",
      "12                 Unknown        3421\n",
      "['Car' 'Home improvement' 'Existing loan takeover' 'Unknown'\n",
      " 'Other, see explanation' 'Remaining debt home' 'Extra spending limit'\n",
      " 'Caravan / Camper' 'Motorcycle' 'Boat' 'Tax payments' 'Business goal'\n",
      " 'Debt restructuring']\n",
      "                 case_goal  case_count\n",
      "2                      Car        9307\n",
      "7         Home improvement        7646\n",
      "5   Existing loan takeover        5574\n",
      "12                 Unknown        3421\n",
      "9   Other, see explanation        2976\n",
      "10     Remaining debt home         835\n",
      "['Other, see explanation' 'Home improvement' 'Car'\n",
      " 'Existing loan takeover' 'Unknown' 'Remaining debt home']\n"
     ]
    }
   ],
   "source": [
    "path_17 = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\bpic2017\\\\bpic2017_treatments.csv\"\n",
    "path_12 = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\bpic2012\\\\bpic2012_treatments.csv\"\n",
    "\n",
    "results_data3 = [pd.read_csv(path_12, sep=\";\"), pd.read_csv(path_17, sep=\";\")]\n",
    "\n",
    "resource_max = 5\n",
    "case_goal_max = 6\n",
    "\n",
    "for log_name in logs:\n",
    "    df = results_data3[logs.index(log_name)]\n",
    "\n",
    "    # for every case_id group, check how whether \"O_CREATED\" occured more than once, and then check if the treatment is 0, if so, add to a counter\n",
    "    # group = df.groupby(case_id_col)\n",
    "    # counter = 0\n",
    "    # treated_counter = 0\n",
    "    # nr_treated_cases = 0\n",
    "    # created_counter = 0\n",
    "    # if log_name == 'bpic2017':\n",
    "    #     activity_to_check_now = \"O_Create Offer\"\n",
    "    # else:\n",
    "    #     activity_to_check_now = \"O_SELECTED\"\n",
    "    # for name, group in group:\n",
    "    #     if group[activity_col].eq(activity_to_check_now).sum() > 1:\n",
    "    #         created_counter += 1\n",
    "    #     if group[\"treatment\"].eq(1).sum() == 1:\n",
    "    #         nr_treated_cases += 1\n",
    "    #     if group[activity_col].eq(activity_to_check_now).sum() > 1 and group['treatment'].eq(1).sum() >= 1:\n",
    "    #         treated_counter += 1\n",
    "    #     if group[activity_col].eq(activity_to_check_now).sum() > 1 and group['treatment'].eq(0).all():\n",
    "    #         print('lol')\n",
    "    #         counter += 1\n",
    "\n",
    "    # for each value in column 'resource', count in how many unique cases it appears, and get this in a separate df with columns 'resource' and 'case_count'\n",
    "    resource_df = df.groupby('resource')[case_id_col].nunique().reset_index()\n",
    "    resource_df.columns = ['resource', 'case_count']\n",
    "    # sort\n",
    "    resource_df = resource_df.sort_values(by='case_count', ascending=False)\n",
    "    # print the top 10\n",
    "    print(resource_df.head(resource_max))\n",
    "    # then make sure that in the resource column of the original df, only the top 10 are specified, and all others are set to 'other'\n",
    "    top_resources = resource_df.head(resource_max)['resource'].tolist()\n",
    "    df['resource'] = df['resource'].apply(lambda x: x if x in top_resources else 'other')\n",
    "    # print the unique values in the resource column\n",
    "    print(df['resource'].unique())\n",
    "\n",
    "    #drop the coluns 'unnamed:_0' and 'case_length'\n",
    "    if 'case_length' in df.columns:\n",
    "        df = df.drop(columns=['case_length'])\n",
    "    if 'unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['unnamed: 0'])\n",
    "    if 'end_time' in df.columns:\n",
    "        df = df.drop(columns=['end_time'])\n",
    "    if 'reg_date' in df.columns:\n",
    "        df = df.drop(columns=['reg_date'])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    if 'case:loangoal' in df.columns:\n",
    "        # if the value is 'Unknown', or 'Not specified', set it to 'Unknown'\n",
    "        df['case:loangoal'] = df['case:loangoal'].apply(lambda x: 'Unknown' if x == 'Not speficied' else x)\n",
    "        # Now do the same for the case loan goal column\n",
    "        case_goal_df = df.groupby('case:loangoal')[case_id_col].nunique().reset_index()\n",
    "        case_goal_df.columns = ['case_goal', 'case_count']\n",
    "        print(case_goal_df)\n",
    "        # sort\n",
    "        case_goal_df = case_goal_df.sort_values(by='case_count', ascending=False)\n",
    "        # print all unique values\n",
    "        print(case_goal_df['case_goal'].unique())\n",
    "        # print the top 10\n",
    "        print(case_goal_df.head(case_goal_max))\n",
    "        # then make sure that in the case:loangoal column of the original df, only the top 10 are specified, and all others are set to 'other'\n",
    "        top_case_goals = case_goal_df.head(case_goal_max)['case_goal'].tolist()\n",
    "        df['case:loangoal'] = df['case:loangoal'].apply(lambda x: x if x in top_case_goals else 'Other, see explanation')\n",
    "        # print the unique values in the case:loangoal column\n",
    "        print(df['case:loangoal'].unique())\n",
    "\n",
    "    # check if the case_nr column is a string or not. It it is a string, take only the 'number' part of the string, and convert it to an integer\n",
    "    if df[case_id_col].dtype == 'O':\n",
    "        df[case_id_col] = df[case_id_col].apply(lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "    name = \"%s_final.csv\" % log_name\n",
    "\n",
    "    # save\n",
    "    import os\n",
    "    results_dir = \"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\%s\" % log_name\n",
    "    df.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\u0166838\\\\OneDrive - KU Leuven\\\\Documents\\\\Doc\\\\Code\\\\ProCause\\\\data\\\\bpic2017\\\\bpic2017_final.csv\", sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PresPMGenerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
